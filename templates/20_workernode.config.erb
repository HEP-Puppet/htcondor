MachineOwner = <%= @machine_owner %>
StartJobs = True
## NUM_CPUS based on heira (if defined), otherwise on facter
NUM_CPUS = <%= @number_of_cpus %>

DETECTED_CPUS = $NUM_CPUS

# custom machine attributes for job matching
# e.g. queues with ARC CE (using condor_requirements=" && NORDUGRID_QUEUE)"
<% if @custom_machine_attributes.any? -%>
<% @custom_machine_attributes.each do |k, v| -%>
<%=k -%> = <%=v %>
<% end -%>
STARTD_ATTRS = <%= @custom_machine_attributes.keys.join(', ') -%>, MachineOwner, StartJobs
<% else -%>
STARTD_ATTRS = MachineOwner, StartJobs
<% end -%>

# custom job attributes for job reporting/monitoring
<% if @custom_job_attributes.any? -%>
<% @custom_job_attributes.each do |k, v| -%>
<%=k -%> = <%=v %>
<% end -%>
STARTD_JOB_ATTRS = <%= @custom_job_attributes.keys.join(', ') -%>, MemoryUsage
<% else -%>
STARTD_JOB_ATTRS = MemoryUsage
<% end -%>

## Permanent way of stopping jobs from starting
STARTD.SETTABLE_ATTRS_ADMINISTRATOR = StartJobs
ENABLE_PERSISTENT_CONFIG = TRUE
PERSISTENT_CONFIG_DIR = /etc/condor/persistent

## Healthcheck
<% if  @enable_healthcheck == true -%>
STARTD_CRON_JOBLIST = $(STARTD_CRON_JOBLIST) WN_HEALTHCHECK
STARTD_CRON_WN_HEALTHCHECK_EXECUTABLE = /usr/local/bin/healhcheck_wn_condor
STARTD_CRON_WN_HEALTHCHECK_PERIOD = 10m
STARTD_CRON_WN_HEALTHCHECK_MODE = periodic
STARTD_CRON_WN_HEALTHCHECK_RECONFIG = false
STARTD_CRON_WN_HEALTHCHECK_KILL = true

## When is this node willing to run jobs?
START = (NODE_IS_HEALTHY =?= True) && (StartJobs =?= True)
<% else -%>
START = TRUE
<% end -%>
SUSPEND = FALSE
CONTINUE = TRUE
WANT_SUSPEND = $(SUSPEND)
KILL = FALSE

## When to nicely stop a job?
## (as opposed to killing it instantaneously)
PREEMPT = FALSE
<% if @partitionable_slots -%>
## Overcommit memory and  quantize the detected memory
MEMORY = <%= @memory_overcommit  %> * quantize( $(DETECTED_MEMORY), 1000 )

## Partitionable slots
NUM_SLOTS = 1
SLOT_TYPE_1               = cpus=100%,mem=100%,auto
NUM_SLOTS_TYPE_1          = 1
SLOT_TYPE_1_PARTITIONABLE = TRUE

## Without this lots of memory (& hence job slots) will be wasted
MODIFY_REQUEST_EXPR_REQUESTMEMORY = quantize(RequestMemory,100)
<% end -%>
## Disable preemption by machine RANK by ranking all jobs equally
RANK = 0

## Niceness of user jobs
JOB_RENICE_INCREMENT = 10

## Since we don't use preemption, make sure slots aren't permanently taken by specific users
CLAIM_WORKLIFE = 0

## Allow jobs time to finish if they need to be preempted (should be same as max walltime allowed)
MAXJOBRETIREMENTTIME = $(HOUR) * 24 * 3

## Update collector at random intervals
UPDATE_INTERVAL = $RANDOM_INTEGER(230, 370)
MASTER_UPDATE_INTERVAL = $RANDOM_INTEGER(230, 370)

## Location of scratch directories
EXECUTE = <%= @pool_home %>/condor

## Writable scratch directories bind mounted in scratch, e.g. for docker / singularity containers. 
## Auto-deleted after the job exits.
MOUNT_UNDER_SCRATCH = "<%= @mount_under_scratch_dirs.flatten.join(", ") %>"

## Make sure jobs have independent PID namespaces
<% if @use_pid_namespaces -%>
USE_PID_NAMESPACES = true
<% else -%>
USE_PID_NAMESPACES = false
<% end -%>

## If the binaries are updated, let any running jobs finish before restarting
MASTER_NEW_BINARY_RESTART=PEACEFUL

## Logs
MAX_MASTER_LOG = 104857600
MAX_NUM_MASTER_LOG = 10

MAX_STARTD_LOG = 104857600
MAX_NUM_STARTD_LOG = 10

<%- if @enable_cgroup -%>
# Enable CGROUP
BASE_CGROUP = <%= @htcondor_cgroup %>
CGROUP_MEMORY_LIMIT = soft
<%- end -%>

## Debugging
#STARTD_DEBUG = D_COMMAND D_FULLDEBUG

##  This macro determines what daemons the condor_master will start and keep its watchful eyes on.
##  The list is a comma or space separated list of subsystem names
DAEMON_LIST = <%= @daemon_list %>
